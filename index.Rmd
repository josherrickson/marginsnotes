---
title: "Marginal Effects"
output:
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    css: style.css
    theme: paper
    source_code: "https://github.com/josherrickson/marginsnotes"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Sidebar {.sidebar}
-------------------------------------

### Layout

On the left sidebar for each page, you'll find a (mostly) software-agnostic discussion of the topic at hand.

The main panels of each page consist of a tabbed series of pages with the code and output in the noted language. On some pages, there will be an
additional Math tab with mathematical details.

Column
-------------------------------------

### Marginal Effects

After running a regression model of some sort, the common way of interpreting the relationships between predictors and the outcome is by interpreting
the regression coefficients. In many situations these interpretations are straightforward, however, in some settings the interpretations can be
tricky, or the coefficients can simply not be interpreted.

These complications arise most commonly when a model involves an interaction or moderation. In these sort of models, interpretation of the
coefficients requires special care due to the relationship between the various interacted predictors, and the set of information obtainable from the
coefficients is often limited without resorting to significant algebra.

Marginal effects offer an improvement over simple coefficient interpretation. **Marginal means** allow you to estimate the average response under a
set of conditions; e.g. the average response for each racial group, or the average response when age and weight are at certain values.

**Marginal slopes** estimate the slope between a predictor and the outcome when other variables are at some specific value; e.g. the average slope on
age within each gender, or the average slope on age when weight is at certain values.

In either case, in addition to estimating these marginal effects, we can easily test hypotheses of equality between them via **pairwise
comparisons**. For example, is the average response different by ethnicity, or is the slope on age different between genders.

If you are familiar with interpreting regression coefficients, and specifically interactions, you may have some idea of how to start addressing all
the above examples. However, to complete answer these research questions would require more than simply a table of regression coefficients. With
marginal effects, one additional set of results can address all these questions.

Finally, marginal effect estimation is a step towards creating informative visualizations of relationships such as interaction plots.


# Data and Libraries

Sidebar {.sidebar}
-------------------------------------

### Data

For all examples, we'll use the "margex" data set from Stata. This data set was designed for examples using `margins` and contains a nice mix of
continuous and categorical variables.

The primary outcome variable will be "y", a continuous variable.

The predictors we'll be using include "group", a categorical variable with 3 levels; "sex", a binary variable; and "age", a continuous variable.

### Libraries

In Stata, everything needed is built-in.

In R, we'll need to load several libraries.

- `emmeans` - The primary package used to estimate marginal effects.
- `interactions` - Used for visualization.
- `haven` - Used to read the "margex" data from Stata into R.


Code {.tabset}
-------------------------------------

### Stata

```{r child = 'data.Rmd'}
```

### R

```{r child = 'R_markdown/data_and_packages.Rmd'}
```

# Categorical Variables {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Marginal Means for Categorical Variables

When we have a single categorical variable (not involved in an interaction) in a model, one level is excluded as a reference or baseline category, and
the coefficients on the other levels represent the average difference between those levels and the reference.

The marginal means for each level of the categorical can be calcuated to estimate the average response for that level.

To obtain this estimate, we assume that every observation belongs to the same group, then predict their outcome in the model as if they were in that
group, and their other predictors were at the observed level. Finally, average over all observations.

#### Example

The regression model has a single predictor, "group", which is the aforementioned 3-level categorical variable. Group 1 is excluded as the reference
category, so the intercept (`_cons` in Stata) estimates the predicted average response in group 1 of 68.4. The coefficients on group 2 and group 3
reflect the difference the predicted average response per group, so the predicted average response in group 2 is 68.4 + .4 = 68.8. Correspondingly,
the average response for group 3 is 68.4 + 5.4 = 73.8.

Notice that the margin means gives us the same predicted average response per group, along with standard errors. (The standard error for group 1's
estimate is the same as the intercept, but the standard error for the two other groups would require additional work.)

The regression output does contain some additional information; namely the coefficients on group 2 and group 3 test the hypotheses respectively that
group 1 and group 2 are equivalent, and that group 1 and group 3 are equivalent; neither of which are provided by the marginal means. However,
[pairwise comparisons](#categorical-variables-1) can obtain these easily, as well as the comparison between groups 2 and 3 which is not provided by
the regression output.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'categorical.Rmd'}
```

### R

```{r child = 'R_markdown/categorical.Rmd'}
```

### Math

Assume a linear regression set up with a single categorical variable, \(G\), with three groups. We fit

\[
  E(Y|G) = \beta_0 + \beta_1g_2 + \beta_2g_3,
\]

where \(g_2\) and \(g_3\) are dummy variables representing membership in groups 2 and 3 respectively (\(g_{2i} = 1\) if observation \(i\) has \(G =
2\), and equal to 0 otherwise.) Since \(g_1\) is not found in the model, it is the reference category.

Therefore, \(\beta_0\) represents the average response among the reference category \(G = 1\). \(\beta_1\) represents the difference in the average
response between groups \(G = 1\) and \(G = 2\). Therefore, \(\beta_0 + \beta_1\) is the average response in group \(G = 2\). A similar argument can
be made about \(\beta_2\) and group 3.


# Interactions between Categorical Variables {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Interactions between Categorical Variables

When we have an interaction of categorical variables, we can estimate the marginal mean in each combination of levels from the two categorical
variables. In other words, think of it as a single categorical variable describing all combinations and it operates similarly - set all observations
to be a particular combination of levels, predict the outcome using their observed covariates, and average.

#### Example

Since this is not that different that a single categorical variable, everything carries over - the intercept in the regression model represents the
reference category (group 1 males) for a predicted average response of 50.6. The coefficients on group 2 and 3 represent the difference between the
predicted average response for group 1 versus groups 2 and 3 respectively, *among males*. The coefficient on female represents the difference between
the predicted average response for males and females, *among group 1*. The coefficients on the interactions represent additional conditional
comparisons. We can use basic arithmetic to estimate the predicted average responses in other groups; for example females in group 2 are estimated by
the sum of all coefficients in female and group 2 added to the intercept - so the female coefficient, the group 2 coefficient, and the female by group
2 interaction, for 50.6 + 21.6 + 11.4 + (-4.9) = 78.7. Looking at the margins output, we see this corresponds.

The discussion about standard errors and hypothesis testing carries forward as well.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'categorical_interaction.Rmd'}
```

### R

```{r child = 'R_markdown/categorical_interaction.Rmd'}
```

### Math

Consider the simplest case, with two binary variables \(Z\) and \(K\). We fit the model

\[
  E(Y|Z,K) = \beta_0 + \beta_1Z + \beta_2K + \beta_3ZK,
\]

where \(ZK = 0\) only if both \(Z = 1\) and \(K = 1\).

This is functionally equivalent to defining a new variable \(L\),

\[
  L = \begin{cases}
  0, & K = 0 \& Z = 0 \\
  1, & K = 0 \& Z = 1 \\
  2, & K = 1 \& Z = 0 \\
  3, & K = 1 \& Z = 1,
  \end{cases}
\]

and fitting the model

\[
  E(Y|L) = \beta_0 + \beta_1l_1 + \beta_2l_2 + \beta_3l_3.
\]

# Continuous Variables {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Marginal Means for Continuous Variables

Marginal means for continuous variables actually operate the same as for categorical variables, in the sense that they are setting all observations to
a specific value of the continuous variable, then all predicted values are averaged.

Typically we obtain the marginal means over several meaningful values of the categorical variable to glimpse at the
relationship. [Plotting](#plotting) is extremely useful and always recommended when marginal effects for continuous variables are desired.

We can of course obtain marginal means with any number of variables fixed; in the example to the right we first obtain marginal means when "age" is at
certain values, then when "age" is at certain values and within each group.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'continuous_marginal_means.Rmd'}
```

### R

```{r child = 'R_markdown/continuous_marginal_means.Rmd'}
```

# Categorical and Continuous Interactions {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Categorical and Continuous Interactions

When there are interactions in the model, the code to obtain marginal means does not change. From the perspective of "fixing some predictors to
specific values and predicting the outcome", whether there are interactions or not do not matter.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'categorical_continuous_interaction.Rmd'}
```

### R

```{r child = 'R_markdown/categorical_continuous_interaction.Rmd'}
```

# Marginal Slopes {data-navmenu="Marginal Slopes"}

Sidebar {.sidebar}
-------------------------------------

### Marginal Slopes

In addition to the marginal means, we can also estimate the marginal slopes. In other words, the predicted average slope on a given predictor, when
all other predictors are at their observed value. When the continuous variable whose marginal slope we estimate is *not* involved in an interaction,
this simply recovers the estimates coefficient from the regression model. This will become more interesting when the continuous variable is involved
in an interaction.

(Note that although we interpret binary variables in a particular way, mathematically they are simply continuous variables [for continuous variables
we often discuss "a 1-unit increase"; for a binary variable, a 1-unit increase is simply 1 versus 0]. Therefore we can in fact obtain marginal slopes
for binary predictors, which is again the estimated coefficient and represents the average predicted difference between groups.)

#### Example

We see in this example that the estimated coefficient on the "age" continuous variable is -0.5; the estimated marginal slope for age is the same -0.5.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'slopes_no_interaction.Rmd'}
```

### R

```{r child = 'R_markdown/slopes_no_interaction.Rmd'}
```

# Categorical by continuous interaction {data-navmenu="Marginal Slopes"}

sidebar {.sidebar}
-------------------------------------

### Marginal Slopes with Interactions

When there is an interaction between a categorical variable and a continuous variable, it is natural to want to estimate the slope at each level of
the categorical variable, the marginal slope. This is very similar to the marginal mean, but instead of the average predicted outcome per group, it's
the average predicted slope per group.

As we mentioned when discussing [marginal means with interactions](#interactions-between-categorical-variables), regression coefficients tell only a
partial story when there is an interaction involved. Regression coefficients will estimate the slope in the reference group and the difference between
the slope in the reference group and other groups, but not explicitly estimate the slope in each group. Marginal slopes does this.

#### Example

Here we interact "sex" (binary) with "age" (continuous). The regression coefficients tell us that the slope among males (the reference group) is
-0.49, and the difference in slopes between groups is -0.022. The marginal mean gives us the slope amongst males again, as well as -0.49 + (-0.02) =
0.51.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'slopes_interaction.Rmd'}
```

### R

```{r child = 'R_markdown/slopes_interaction.Rmd'}
```


# Categorical Variables {data-navmenu="Pairwise Comparisons"}

Sidebar {.sidebar}
-------------------------------------

### Pairwise Comparisons of Categorical Variables


Code {.tabset}
-------------------------------------

### Stata

```{r child = 'pairwise_categorical.Rmd'}
```

### R

```{r child = 'R_markdown/pairwise_categorical.Rmd'}
```

# Categorical Interactions {data-navmenu="Pairwise Comparisons"}

Sidebar {.sidebar}
-------------------------------------

### Pairwise Comparisons of Categorical Interactions


Column {.tabset}
-------------------------------------

### Stata

```{r child = 'pairwise_categorical_interaction.Rmd'}
```

### R

```{r child = 'R_markdown/pairwise_categorical_interaction.Rmd'}
```

# Plotting

Sidebar {.sidebar}
-------------------------------------

### Plotting


Code {.tabset}
-------------------------------------

### Stata

```{r child = 'plotting.Rmd'}
```

### R

```{r child = 'R_markdown/plotting.Rmd'}
```
