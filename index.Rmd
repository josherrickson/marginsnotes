---
title: "Marginal Effects"
output:
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    css: style.css
    theme: paper
    source_code: "https://github.com/josherrickson/marginsnotes"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Sidebar {.sidebar}
-------------------------------------

### Layout

On the left sidebar for each page, you'll find a (mostly)
software-agnostic discussion of the topic at hand.

The main panels of each page consist of a tabbed series of pages with
the code and output in the noted language. On some pages, there will
be an additional Math tab with mathematical details.

Column
-------------------------------------

### Marginal Effects

After running a regression model of some sort, the common way of
interpreting the relationships between predictors and the outcome is
by interpreting the regression coefficients. In many situations these
interpretations are straightforward, however, in some settings the
interpretations can be tricky, or the coefficients can simply not be
interpreted.

These complications arise most commonly when a model involves an
interaction or moderation. In these sort of models, interpretation of
the coefficients requires special care due to the relationship between
the various interacted predictors, and the set of information
obtainable from the coefficients is often limited without resorting to
significant algebra.

Marginal effects offer an improvement over simple coefficient
interpretation. **Marginal means** allow you to estimate the average
response under a set of conditions; e.g. the average response for each
racial group, or the average response when age and weight are at
certain values.

**Marginal slopes** estimate the slope between a
predictor and the outcome when other variables are at some specific
value; e.g. the average slope on age within each gender, or the
average slope on age when weight is at certain values.

In either case, in addition to estimating these marginal effects, we
can easily test hypotheses of equality between them via **pairwise
comparisons**. For example, is the average response different by
ethnicity, or is the slope on age different between genders.

If you are familiar with interpreting regression coefficients, and
specifically interactions, you may have some idea of how to start
addressing all the above examples. However, to complete answer these
research questions would require more than simply a table of
regression coefficients. With marginal effects, one additional set of
results can address all these questions.

Finally, marginal effect estimation is a step towards creating
informative visualizations of relationships such as interaction plots.


# Data and Libraries

Sidebar {.sidebar}
-------------------------------------

### Data

For all examples, we'll use the "margex" data set from Stata. This
data set was designed for examples using `margins` and contains a nice
mix of continuous and categorical variables.

The primary outcome variable will be "y", a continuous variable.

The predictors we'll be using include "group", a categorical variable
with 3 levels; "sex", a binary variable; and "age", a continuous
variable.

### Libraries

In Stata, everything needed is built-in.

In R, we'll need to load several libraries.

- `emmeans` - The primary package used to estimate marginal effects.
- `interactions` - Used for visualization.
- `haven` - Used to read the "margex" data from Stata into R.


Code {.tabset}
-------------------------------------

### Stata

```{r child = 'data.Rmd'}
```

### R

```{r child = 'R_markdown/data_and_packages.Rmd'}
```

# Categorical Variables {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Marginal Means for Categorical Variables

When we have a single categorical variable (not involved in an
interaction) in a model, one level is excluded as a reference or
baseline category, and the coefficients on the other levels represent
the average difference between those levels and the reference.

The marginal means for each level of the categorical can be calcuated
to estimate the average response for that level.

To obtain this estimate, we assume that every observation belongs to
the same group, then predict their outcome in the model as if they
were in that group, and their other predictors were at the observed
level. Finally, average over all observations.

#### Example

The regression model has a single predictor, "group", which is the
aforementioned 3-level categorical variable. Group 1 is excluded as
the reference category, so the intercept (`_cons` in Stata) estimates
the predicted average response in group 1 of 68.4. The coefficients on
group 2 and group 3 reflect the difference the predicted average
response per group, so the predicted average response in group 2 is
68.4 + .4 = 68.8. Correspondingly, the average response for group 3 is
68.4 + 5.4 = 73.8.

Notice that the margin means gives us the same predicted average
response per group, along with standard errors. (The standard error
for group 1's estimate is the same as the intercept, but the standard
error for the two other groups would require additional work.)

The regression output does contain some additional information; namely
the coefficients on group 2 and group 3 test the hypotheses
respectively that group 1 and group 2 are equivalent, and that group 1
and group 3 are equivalent; neither of which are provided by the
marginal means. However, [pairwise
comparisons](#categorical-variables-1) can obtain these easily, as well
as the comparison between groups 2 and 3 which is not provided by the
regression output.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'categorical.Rmd'}
```

### R

```{r child = 'R_markdown/categorical.Rmd'}
```

### Math

Assume a linear regression set up with a single categorical variable,
\(G\), with three groups. We fit

\[
  E(Y|G) = \beta_0 + \beta_1g_2 + \beta_2g_3,
\]

where \(g_2\) and \(g_3\) are dummy variables representing membership
in groups 2 and 3 respectively (\(g_{2i} = 1\) if observation \(i\)
has \(G = 2\), and equal to 0 otherwise.) Since \(g_1\) is not found
in the model, it is the reference category.

Therefore, \(\beta_0\) represents the average response among the
reference category \(G = 1\). \(\beta_1\) represents the difference in
the average response between groups \(G = 1\) and \(G =
2\). Therefore, \(\beta_0 + \beta_1\) is the average response in group
\(G = 2\). A similar argument can be made about \(\beta_2\) and group
3.


# Interactions with Categorical Variables {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Interactions with Categorical Variables

When we have an interaction of categorical variables, we can estimate
the marginal mean in each combination of levels from the two
categorical variables. In other words, think of it as a single
categorical variable describing all combinations and it operates
similarly - set all observations to be a particular combination of
levels, predict the outcome using their observed covariates, and
average.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'categorical_interaction.Rmd'}
```

### R

```{r child = 'R_markdown/categorical_interaction.Rmd'}
```

### Math

Consider the simplest case, with two binary variables \(Z\) and
\(K\). We fit the model

\[
  E(Y|Z,K) = \beta_0 + \beta_1Z + \beta_2K + \beta_3ZK,
\]

where \(ZK = 0\) only if both \(Z = 1\) and \(K = 1\).

This is functionally equivalent to defining a new variable \(L\),

\[
  L = \begin{cases}
  0, & K = 0 \& Z = 0 \\
  1, & K = 0 \& Z = 1 \\
  2, & K = 1 \& Z = 0 \\
  3, & K = 1 \& Z = 1,
  \end{cases}
\]

and fitting the model

\[
  E(Y|L) = \beta_0 + \beta_1l_1 + \beta_2l_2 + \beta_3l_3.
\]

# Continuous Variables {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Marginal Means for Continuous Variables

Marginal means for continuous variables actually operate the same as
for categorical variables, in the sense that they are setting all
observations to a specific value of the continuous variable, then all
predicted values are averaged.

Typically we obtain the marginal means over several meaningful values
of the categorical variable to glimpse at the
relationship. [Plotting](#plotting) is extremely useful and always
recommended when marginal effects for continuous variables are
desired.

We can of course obtain marginal means with any number of variables
fixed; in the example to the right we first obtain marginal means when
"age" is at certain values, then when "age" is at certain values and
within each group.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'continuous_marginal_means.Rmd'}
```

### R

```{r child = 'R_markdown/continuous_marginal_means.Rmd'}
```

# Categorical and Continuous Interactions {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Categorical and Continuous Interactions

When there are interactions in the model, the code to obtain marginal
means does not change. From the perspective of "fixing some predictors
to specific values and predicting the outcome", whether there are
interactions or not do not matter.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'categorical_continuous_interaction.Rmd'}
```

### R

```{r child = 'R_markdown/categorical_continuous_interaction.Rmd'}
```

# No Interactions {data-navmenu="Marginal Slopes"}

Sidebar {.sidebar}
-------------------------------------

### Marginal Slopes with No Interactions

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'slopes_no_interaction.Rmd'}
```

### R

```{r child = 'R_markdown/slopes_no_interaction.Rmd'}
```

# Interactions {data-navmenu="Marginal Slopes"}

sidebar {.sidebar}
-------------------------------------

### Marginal Slopes with Interactions


Code {.tabset}
-------------------------------------

### Stata

```{r child = 'slopes_interaction.Rmd'}
```

### R

```{r child = 'R_markdown/slopes_interaction.Rmd'}
```


# Categorical Variables {data-navmenu="Pairwise Comparisons"}

Sidebar {.sidebar}
-------------------------------------

### Pairwise Comparisons of Categorical Variables


Code {.tabset}
-------------------------------------

### Stata

```{r child = 'pairwise_categorical.Rmd'}
```

### R

```{r child = 'R_markdown/pairwise_categorical.Rmd'}
```

# Categorical Interactions {data-navmenu="Pairwise Comparisons"}

Sidebar {.sidebar}
-------------------------------------

### Pairwise Comparisons of Categorical Interactions


Column {.tabset}
-------------------------------------

### Stata

```{r child = 'pairwise_categorical_interaction.Rmd'}
```

### R

```{r child = 'R_markdown/pairwise_categorical_interaction.Rmd'}
```

# Plotting

Sidebar {.sidebar}
-------------------------------------

### Plotting


Code {.tabset}
-------------------------------------

### Stata

```{r child = 'plotting.Rmd'}
```

### R

```{r child = 'R_markdown/plotting.Rmd'}
```
