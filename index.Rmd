---
title: "Marginal Effects"
output:
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    css: style.css
    theme: paper
    source_code: "https://github.com/josherrickson/marginsnotes"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Sidebar {.sidebar}
-------------------------------------

### Layout

On the left sidebar for each page, you'll find a (mostly) software-agnostic discussion of the topic at hand.

The main panels of each page consist of a tabbed series of pages with the code and output in the noted language. On some pages, there will be an
additional Math tab with mathematical details.

Column
-------------------------------------

### Marginal Effects

After running a regression model of some sort, the common way of interpreting the relationships between predictors and the outcome is by interpreting
the regression coefficients. In many situations these interpretations are straightforward, however, in some settings the interpretations can be
tricky, or the coefficients can simply not be interpreted.

These complications arise most commonly when a model involves an interaction or moderation. In these sort of models, interpretation of the
coefficients requires special care due to the relationship between the various interacted predictors, and the set of information obtainable from the
coefficients is often limited without resorting to significant algebra.

Marginal effects offer an improvement over simple coefficient interpretation. **Marginal means** allow you to estimate the average response under a
set of conditions; e.g. the average response for each racial group, or the average response when age and weight are at certain values.

**Marginal slopes** estimate the slope between a predictor and the outcome when other variables are at some specific value; e.g. the average slope on
age within each gender, or the average slope on age when weight is at certain values.

In either case, in addition to estimating these marginal effects, we can easily test hypotheses of equality between them via **pairwise
comparisons**. For example, is the average response different by ethnicity, or is the slope on age different between genders.

If you are familiar with interpreting regression coefficients, and specifically interactions, you may have some idea of how to start addressing all
the above examples. However, to complete answer these research questions would require more than simply a table of regression coefficients. With
marginal effects, one additional set of results can address all these questions.

Finally, marginal effect estimation is a step towards creating informative visualizations of relationships such as interaction plots.


# Data and Libraries

Sidebar {.sidebar}
-------------------------------------

### Data

For all examples, we'll use the "margex" data set from Stata. This data set was designed for examples using `margins` and contains a nice mix of
continuous and categorical variables.

The primary outcome variable will be "y", a continuous variable.

The predictors we'll be using include "group", a categorical variable with 3 levels; "sex", a binary variable; and "age", a continuous variable.

### Libraries

In Stata, everything needed is built-in.

For R, I've implemented the marginal effects estimates using both the `emmeans` and `ggeffects` packages. You'll see each in it's own tab on the right. In addition, there is an additional package, `interactions`, which we use for plotting.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'data.Rmd'}
```

### R Data

```{r child = 'R_markdown/data.Rmd'}
```

### R Packages

```{r child = 'R_markdown/packages.Rmd'}
```

# Categorical Variables {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Marginal Means for Categorical Variables

When we have a single categorical variable (not involved in an interaction) in a model, one level is excluded as a reference or baseline category, and
the coefficients on the other levels represent the average difference between those levels and the reference.

The marginal means for each level of the categorical can be calcuated to estimate the average response for that level.

To obtain this estimate, we assume that every observation belongs to the same group, then predict their outcome in the model as if they were in that
group, and their other predictors were at the observed level. Finally, average over all observations.

#### Example

The regression model has a single predictor, "group", which is the aforementioned 3-level categorical variable. Group 1 is excluded as the reference
category, so the intercept (`_cons` in Stata) estimates the predicted average response in group 1 of 68.4. The coefficients on group 2 and group 3
reflect the difference the predicted average response per group, so the predicted average response in group 2 is 68.4 + .4 = 68.8. Correspondingly,
the average response for group 3 is 68.4 + 5.4 = 73.8.

Notice that the margin means gives us the same predicted average response per group, along with standard errors. (The standard error for group 1's
estimate is the same as the intercept, but the standard error for the two other groups would require additional work.)

The regression output does contain some additional information; namely the coefficients on group 2 and group 3 test the hypotheses respectively that
group 1 and group 2 are equivalent, and that group 1 and group 3 are equivalent; neither of which are provided by the marginal means. However,
[pairwise comparisons](#categorical-variables-1) can obtain these easily, as well as the comparison between groups 2 and 3 which is not provided by
the regression output.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'mm_cat.Rmd'}
```

### R (`emmeans`)

```{r child = 'R_markdown/mm_emmeans_cat.Rmd'}
```

### R (`ggeffects`)

```{r child = 'R_markdown/mm_ggeffects_cat.Rmd'}
```

### Math

Assume a linear regression set up with a single categorical variable, \(G\), with three groups. We fit

\[
  E(Y|G) = \beta_0 + \beta_1g_2 + \beta_2g_3,
\]

where \(g_2\) and \(g_3\) are dummy variables representing membership in groups 2 and 3 respectively (\(g_{2i} = 1\) if observation \(i\) has \(G =
2\), and equal to 0 otherwise.) Since \(g_1\) is not found in the model, it is the reference category.

Therefore, \(\beta_0\) represents the average response among the reference category \(G = 1\). \(\beta_1\) represents the difference in the average
response between groups \(G = 1\) and \(G = 2\). Therefore, \(\beta_0 + \beta_1\) is the average response in group \(G = 2\). A similar argument can
be made about \(\beta_2\) and group 3.


# Interactions between Categorical Variables {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Interactions between Categorical Variables

When we have an interaction of categorical variables, we can estimate the marginal mean in each combination of levels from the two categorical
variables. In other words, think of it as a single categorical variable describing all combinations and it operates similarly - set all observations
to be a particular combination of levels, predict the outcome using their observed covariates, and average.

#### Example

Since this is not that different that a single categorical variable, everything carries over - the intercept in the regression model represents the
reference category (group 1 males) for a predicted average response of 50.6. The coefficients on group 2 and 3 represent the difference between the
predicted average response for group 1 versus groups 2 and 3 respectively, *among males*. The coefficient on female represents the difference between
the predicted average response for males and females, *among group 1*. The coefficients on the interactions represent additional conditional
comparisons. We can use basic arithmetic to estimate the predicted average responses in other groups; for example females in group 2 are estimated by
the sum of all coefficients in female and group 2 added to the intercept - so the female coefficient, the group 2 coefficient, and the female by group
2 interaction, for 50.6 + 21.6 + 11.4 + (-4.9) = 78.7. Looking at the margins output, we see this corresponds.

The discussion about standard errors and hypothesis testing carries forward as well.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'mm_cat_int.Rmd'}
```

### R (`emmeans`)

```{r child = 'R_markdown/mm_emmeans_cat_int.Rmd'}
```

### R (`ggeffects`)

```{r child = 'R_markdown/mm_ggeffects_cat_int.Rmd'}
```

### Math

Consider the simplest case, with two binary variables \(Z\) and \(K\). We fit the model

\[
  E(Y|Z,K) = \beta_0 + \beta_1Z + \beta_2K + \beta_3ZK,
\]

where \(ZK = 1\) only if both \(Z = 1\) and \(K = 1\).

This is functionally equivalent to defining a new variable \(L\),

\[
  L = \begin{cases}
  0, & K = 0 \& Z = 0 \\
  1, & K = 0 \& Z = 1 \\
  2, & K = 1 \& Z = 0 \\
  3, & K = 1 \& Z = 1,
  \end{cases}
\]

and fitting the model

\[
  E(Y|L) = \beta_0 + \beta_1l_1 + \beta_2l_2 + \beta_3l_3.
\]

# Continuous Variables {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Marginal Means for Continuous Variables

Marginal means for continuous variables actually operate the same as for categorical variables, in the sense that they are setting all observations to
a specific value of the continuous variable, then all predicted values are averaged.

Typically we obtain the marginal means over several meaningful values of the categorical variable to glimpse at the
relationship. [Plotting](#plotting) is extremely useful and always recommended when marginal effects for continuous variables are desired.

We can of course obtain marginal means with any number of variables fixed; in the example to the right we first obtain marginal means when "age" is at
certain values, then when "age" is at certain values and within each group.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'mm_con.Rmd'}
```

### R (`emmeans`)

```{r child = 'R_markdown/mm_emmeans_con.Rmd'}
```

### R (`ggeffects`)

```{r child = 'R_markdown/mm_ggeffects_con.Rmd'}
```

### Math

Consider a model with a single continuous predictor, plus a control variable (which may be continuous or categorical).

\[
  E(Y|X, Z) = \beta_0 + \beta_1X + \beta_2Z.
\]

For any given value of \(X\), it is easy to compute the marginal mean for a given individual. For example,

\[
  E(Y|X = 2, Z) = \beta_0 + 2\beta_1 + \beta_2Z.
\]

Therefore we can easily compute \(E(y_i|x = 2, z = z_i)\) for each individual (the predicted outcome if each individual had a value of \(X = 2\), with
their other values at the observed value) and average them.


# Categorical and Continuous Interactions {data-navmenu="Marginal Means"}

Sidebar {.sidebar}
-------------------------------------

### Categorical and Continuous Interactions

When there are interactions in the model, the code to obtain marginal means does not change. From the perspective of "fixing some predictors to
specific values and predicting the outcome", whether there are interactions or not do not matter.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'mm_cat_con_int.Rmd'}
```

### R (`emmeans`)

```{r child = 'R_markdown/mm_emmeans_cat_con_int.Rmd'}
```

### R (`ggeffects`)

```{r child = 'R_markdown/mm_ggeffects_cat_con_int.Rmd'}
```

# Marginal Slopes {data-navmenu="Marginal Slopes"}

Sidebar {.sidebar}
-------------------------------------

### Marginal Slopes

In addition to the marginal means, we can also estimate the marginal slopes. In other words, the predicted average slope on a given predictor, when
all other predictors are at their observed value. When the continuous variable whose marginal slope we estimate is *not* involved in an interaction,
this simply recovers the estimates coefficient from the regression model. This will become more interesting when the continuous variable is involved
in an interaction.

(Note that although we interpret binary variables in a particular way, mathematically they are simply continuous variables [for continuous variables
we often discuss "a 1-unit increase"; for a binary variable, a 1-unit increase is simply 1 versus 0]. Therefore we can in fact obtain marginal slopes
for binary predictors, which is again the estimated coefficient and represents the average predicted difference between groups.)

#### Example

We see in this example that the estimated coefficient on the "age" continuous variable is -0.5; the estimated marginal slope for age is the same -0.5.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'ms_con.Rmd'}
```

### R (`emmeans`)

```{r child = 'R_markdown/ms_emmeans_con.Rmd'}
```

### Math

The Stata `margins` command to obtain marginal means includes the "`dydx`" option, which points to derivative - and indeed, this is exactly what is computer. If we have a basic regression model,

\[
  E(Y|X) = \beta_0 + \beta_1X
\]

taking the derivative with respect to \(X\) will obtain \(\beta_1\), which is the estimated coefficient.

If \(X\) enters the model in a more complex way, say a polynomial term:

\[
  E(Y|X) = \beta_0 + \beta_1X + \beta_2X^2
\]

now the derivative is \(\beta_1 + 2\beta_2X\). Similar to marginal means, where the predicted outcome was estimated for each individual then those
outcomes averaged, here the derivative is estimated plugging in each observation's value of \(X\), then averaged.

# Categorical by continuous interaction {data-navmenu="Marginal Slopes"}

sidebar {.sidebar}
-------------------------------------

### Marginal Slopes with Interactions

When there is an interaction between a categorical variable and a continuous variable, it is natural to want to estimate the slope at each level of
the categorical variable, the marginal slope. This is very similar to the marginal mean, but instead of the average predicted outcome per group, it's
the average predicted slope per group.

As we mentioned when discussing [marginal means with interactions](#interactions-between-categorical-variables), regression coefficients tell only a
partial story when there is an interaction involved. Regression coefficients will estimate the slope in the reference group and the difference between
the slope in the reference group and other groups, but not explicitly estimate the slope in each group. Marginal slopes does this.

#### Example

Here we interact "sex" (binary) with "age" (continuous). The regression coefficients tell us that the slope among males (the reference group) is
-0.49, and the difference in slopes between groups is -0.022. The marginal mean gives us the slope amongst males again, as well as -0.49 + (-0.02) =
0.51 as the slope among females.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'ms_con_int.Rmd'}
```

### R (`emmeans`)

```{r child = 'R_markdown/ms_emmeans_con_int.Rmd'}
```

### Math

Let's assume we have a binary variable, \(Z\), interacted with a continuous variable, \(X\).

\[
  E(Y|X,Z) = \beta_0 + \beta_1X + \beta_2Z + \beta_3XZ
\]

Here we're combining the math from marginal effects with marginal slopes. First, we generate two equations, one for \(Z = 0\) and one for \(Z = 1\):

\begin{align*}
  E(Y|X, Z = 0) & = \beta_0 + \beta_1X \\
  E(Y|X, Z = 1) & = \beta_0 + \beta_1X + \beta_2 + \beta_3X = (\beta_0 + \beta_2) + (\beta_1 + \beta_3)X.
\end{align*}

Then when we differentiate each with respect to \(X\), we obtain \(\beta_1\) and \((\beta_1 + \beta_3)\) respectively.


# Categorical Variables {data-navmenu="Pairwise Comparisons"}

Sidebar {.sidebar}
-------------------------------------

### Pairwise Comparisons of Categorical Variables

Estimating marginal means and slopes is very convenient, but usually we'll require addressing a specific research question determining whether two
marginal means or slopes are statistically distinguishable. We saw that marginal means for categorical variables are simple functions of the
regressino coefficients, so testing for equality between groups is eqiuvalent to testing for whether the model found a difference between groups.

To test for these differences, we'll be obtaining all pairwise differences.

Keeping track of the direction of these differences can be cumbersome; I'd recommend looking at both marginal means and pairwise comparisons to keep
the directionality straight.

#### Example

We return to our simple model with a single 3-level predictor. In the regression model, the coefficients on group 2 and group 3 are actually exactly
what we want, however, note that the comparison between groups 2 and 3 is not represented. This gap grows exponentially as the number of groups
increases; with 4 groups, we see 3 of the 6 possible pairwise comparisons, and with 6 groups we see only 4 of the 15 possible pairwise comparisons.

In the pairwise comparisons output, we see the 2 vs 1 and 3 vs 1 comparisons replicated, as well as the comparison of 2 versus 3. From here we can
interpret that group 3 is statistically distinguishable from the other groups.

Note that you can get seemingly contradictory results - group A is statistically distinguishable from B, B is statistically distinguishable from C,
but A and C are not statistically distinguishable. These results are not incorrect, and often occur when group B's marginal mean is midway between A
and C.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'pw_cat.Rmd'}
```

### R (`emmeans`)

```{r child = 'R_markdown/pw_emmeans_cat.Rmd'}
```

# Categorical Interactions {data-navmenu="Pairwise Comparisons"}

Sidebar {.sidebar}
-------------------------------------

### Pairwise Comparisons of Categorical Interactions

Since interactions of categorical variables can be thought of as just a single categorical variable, pairwise comparisons between all permutations of
the categorical variables works the same.

Column {.tabset}
-------------------------------------

### Stata

```{r child = 'pw_cat_int.Rmd'}
```

### R (`emmeans`)

```{r child = 'R_markdown/pw_emmeans_cat_int.Rmd'}
```

# Marginal Slopes {data-navmenu="Pairwise Comparisons"}

Sidebar {.sidebar}
-------------------------------------

### Pairwise Comparisons of Marginal Slopes

Pairwise comparisons between marginal slopes works essentially the same; once we can estimate the marginal slopes we want, we can test for equality
between them.

#### Example

As with the [categorical by categorical interaction](#categorical-interactions), we see that the regression coefficients on the interaction terms are
replicated in the pairwise comparisons, but we also get the additional comparison of the slopes in groups 2 versus 3.

Column {.tabset}
-------------------------------------

### Stata

```{r child = 'pw_cat_con_int.Rmd'}
```

### R (`emmeans`)

```{r child = 'R_markdown/pw_emmeans_cat_con_int.Rmd'}
```

# Plotting

Sidebar {.sidebar}
-------------------------------------

### Plotting

Plotting the results from marginal means and slopes, most commonly exemplified in interactions plots, is where the differences between Stata and R
start to show. Stata can plot the results of any `margins` call, with varying levels of usefulness. For each particular variation of the plot, we may
need to find a separate way to plot it in R.

Interaction plots involved plotting the marginal means over several values of two (or more) predictor variables to examine the interaction in more
detail. One predictor is plotted along the X-axis; the other predictor(s) are used to produce multiple lines. If the secondary predictor(s) are
categorical, usually one line is plotted per group; if the secondary predictor(s) is continuous, then lines are drawn at several meaningful values of
the predictor.

#### Example

Here we again model the interaction of group with age. From the regression coefficients and even the marginal means, it's hard to really identify the
full pattern. However, looking at the plots, we can see that amongst younger individuals, group 1 has the highest predicted outcome, whereas that
reverses amongst the elderly with group 1 having the lowest predicted outcome, or at least no difference than the other groups.

Another way to interpret the plot would be that we see a decline in the outcome as individuals age, though the decline is more sharp in group 1 than
the other groups.

Code {.tabset}
-------------------------------------

### Stata

```{r child = 'plotting.Rmd'}
```

### R (`interactions`)

```{r child = 'R_markdown/plotting_interactions.Rmd'}
```
